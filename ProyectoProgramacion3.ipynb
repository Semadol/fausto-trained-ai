{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "K2f5jJatXF5-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from string import punctuation\n",
        "\n",
        "contenido = requests.get('https://www.gutenberg.org/cache/epub/68566/pg68566.txt').text\n",
        "open('data/fausto.txt', 'w', encoding=\"utf-8\").write(contenido)\n",
        "\n",
        "sequence_length = 100\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 30\n",
        "\n",
        "# Tenemos el path del archivo de datos\n",
        "\n",
        "FILE_PATH = \"data/fausto.txt\"\n",
        "BASENAME = os.path.basename(FILE_PATH)\n",
        "\n",
        "# Leemos la data\n",
        "text = open(FILE_PATH, encoding='utf-8').read()\n",
        "# removemos las letras mayusculas para un estilo mas uniforme\n",
        "text = text.lower()\n",
        "# removemos puntuacion\n",
        "text = text.translate(str.maketrans('', '', punctuation))\n",
        "text = text.translate(str.maketrans('', '', '¡ª«·»¿ßáäæèéëíñóöúü—‘’“”•™﻿'))\n",
        "\n",
        "\n",
        "# Esto nos ayuda basicamente a hacer que el entrenamiento sea mas rapido al reducir el vocabulario y hacer mas facil de digerir el texto\n",
        "\n",
        "# Veamos algunos stats del dataset\n",
        "\n",
        "n_chars = len(text)\n",
        "vocab = ''.join(sorted(set(text)))\n",
        "print('unique_chars:', vocab)\n",
        "n_unique_chars = len(vocab)\n",
        "print('Numero de caracteres:', n_chars)\n",
        "print('Numero de caracteres unicos:', n_unique_chars)\n",
        "\n",
        "# Hacemos dos diccionarios, dado que tenemos un string con todos los caracteres unicos de nuestro dataset, podemos hacer un diccionario que mapee a cada caracter un numero y viceversa\n",
        "\n",
        "char2int = {c: i for i, c in enumerate(vocab)}\n",
        "int2char = {i: c for i, c in enumerate(vocab)}\n",
        "\n",
        "# Los guardamos en un archivo\n",
        "pickle.dump(char2int, open(f'{BASENAME}-char2int.pickle', 'wb'))\n",
        "pickle.dump(int2char, open(f'{BASENAME}-int2char.pickle', 'wb'))\n",
        "\n",
        "# Ahora vamos a codificar nuestro dataset, o sea, convertir cada caracter en su entero correspondiente\n",
        "\n",
        "encoded_text = np.array([char2int[c] for c in text])\n",
        "\n",
        "# Construimos un tf.data.Dataset para nuestro encoded_text, si necesitamos escalar nuestro codigo a datasets mas grandes\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "\n",
        "for char in char_dataset.take(8):\n",
        "  print(char.numpy(), int2char[char.numpy()])\n",
        "\n",
        "# Ahora construimos nuestras oraciones, queremos que cada muestra de entrada sea la secuencia de caracteres de longitud sequence_length, y para eso usamos el metodo batch de tf.data.Dataset\n",
        "\n",
        "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
        "\n",
        "# Ahora lo mostramos\n",
        "\n",
        "for sequence in sequences.take(2):\n",
        "  print(''.join([int2char[i] for i in sequence.numpy()]))\n",
        "\n",
        "# Preparamos nuestros inputs y targets, necesitamos un modo de convertir una muestra (secuencia de caracteres) en multiples muestras. Y para eso podemos utilizar el metodo flat_map()\n",
        "\n",
        "def split_sample(sample):\n",
        "  ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
        "  for i in range(1, (len(sample)-1) // 2):\n",
        "    input_ = sample[i: i+sequence_length]\n",
        "    target = sample[i+sequence_length]\n",
        "    #Extendemos el datasete con concatenación\n",
        "    other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
        "    ds = ds.concatenate(other_ds)\n",
        "  return ds\n",
        "\n",
        "#Y ahora preparamos inputs y targets\n",
        "\n",
        "dataset = sequences.flat_map(split_sample)\n",
        "\n",
        "#Esto basicamente nos entrega una tupla de inputs y targets, donde conseguimos una gran cantidad de muestras de entrenamiento, y concatenamos para añadirlas juntas\n",
        "\n",
        "# Hagamos entonces one-hot code de los inputs y las labels (targets)\n",
        "\n",
        "def one_hot_samples(input_, target):\n",
        "  # Por ejemplo, de tener el caracter d (que se encuentra codificado como 3, con 5 caracteres unicos)\n",
        "  # Eso nos retorna el vector: [0, 0, 0, 1, 0], dado que 'd' es el 4to caracter\n",
        "  return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
        "\n",
        "dataset = dataset.map(one_hot_samples)\n",
        "\n",
        "# ahora hemos usado el conveniente metodo \"map()\", para hacer one-hot encode en cada muestra de nuestro dataset.\n",
        "\n",
        "# print las primeras 2 muestras\n",
        "for element in dataset.take(2):\n",
        "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
        "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
        "    print(\"Input shape:\", element[0].shape)\n",
        "    print(\"Target shape:\", element[1].shape)\n",
        "    print(\"=\"*50, \"\\n\")\n",
        "\n",
        "# repetimos, cambiamos y juntamos el dataset\n",
        "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True) # Con drop_remainder = True para eliminar las muestras con menor tamaño que el batch size\n",
        "\n",
        "# Armamos el modelo, el cual basicamente tiene dos capas LSTM con un numero de 128 unidades de LSTM arbitrario.\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(n_unique_chars, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "# Definimos el path del modelo\n",
        "\n",
        "model_weights_path = f'results/{BASENAME}-{sequence_length}.h5'\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Entrenamos al modelo\n",
        "\n",
        "#Hacemos la carpeta results si todavia no existe\n",
        "\n",
        "if not os.path.isdir(\"results\"):\n",
        "  os.mkdir(\"results\")\n",
        "\n",
        "#Entrenamos al modelo\n",
        "\n",
        "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)\n",
        "\n",
        "# Guardamos el modelo\n",
        "\n",
        "model.save(model_weights_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Activation\n",
        "import os\n",
        "\n",
        "sequence_length = 100\n",
        "# dataset file path\n",
        "FILE_PATH = \"data/fausto.txt\"\n",
        "# FILE_PATH = \"data/python_code.py\"\n",
        "BASENAME = os.path.basename(FILE_PATH)\n",
        "# Ahora probemos a generar nuevo texto\n",
        "\n",
        "# Como necesitamos una muestra, tomemos una semilla o alguna sentencia de la data de entrenamiento.\n",
        "\n",
        "seed = \"\"\n",
        "\n",
        "char2int = pickle.load(open(f'{BASENAME}-char2int.pickle', 'rb'))\n",
        "int2char = pickle.load(open(f'{BASENAME}-int2char.pickle', 'rb'))\n",
        "vocab_size = len(char2int)\n",
        "\n",
        "# Construimos el modelo nuevamente\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, vocab_size), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "#Y cargamos el set optimo de pesos del modelo.\n",
        "\n",
        "model.load_weights(f'results/{BASENAME}-{sequence_length}.h5')\n",
        "\n",
        "#y generamos\n",
        "\n",
        "seed = input(\"ingrese una entrada de la cual se va a generar el texto de manera predictiva\\n(minusculas solamente, espacios permitidos y numeros tambien)\")\n",
        "s = seed\n",
        "n_chars = 400\n",
        "\n",
        "#Generamos 400 caracteres\n",
        "\n",
        "generated = ''\n",
        "\n",
        "for i in tqdm.tqdm(range(n_chars), 'Generando texto'):\n",
        "  #Hagamos la input de entrada\n",
        "  X = np.zeros((1, sequence_length, vocab_size))\n",
        "  for t, char in enumerate(seed):\n",
        "    X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1\n",
        "#Predecimos el siguiente caracter\n",
        "  predicted = model.predict(X, verbose=0)[0]\n",
        "  #Convertimos el vector a un entero\n",
        "  next_index = np.argmax(predicted)\n",
        "  #convertimos el entero a un caracter\n",
        "  next_char = int2char[next_index]\n",
        "  #añadimos el caracter a los resultados\n",
        "  generated += next_char\n",
        "  #Cambiamos la seed y el caracter predicho\n",
        "\n",
        "  seed = seed[1:] + next_char\n",
        "\n",
        "print('Seed:', s)\n",
        "print('Texto generado:')\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "WTfiLdrZKQMV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}