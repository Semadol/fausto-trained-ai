{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "K2f5jJatXF5-",
        "outputId": "4f41821f-e29c-4f06-ecac-eeb28fab7190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique_chars: \n",
            " 0123456789abcdefghijklmnopqrstuvwxyz\n",
            "Numero de caracteres: 313665\n",
            "Numero de caracteres unicos: 38\n",
            "31 t\n",
            "19 h\n",
            "16 e\n",
            "1  \n",
            "27 p\n",
            "29 r\n",
            "26 o\n",
            "21 j\n",
            "the project gutenberg ebook of fausto primera parte\n",
            "    \n",
            "this ebook is for the use of anyone anywhere in the united states and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "w\n",
            "hatsoever you may copy it give it away or reuse it under the terms\n",
            "of the project gutenberg license included with this ebook or online\n",
            "at wwwgutenbergorg if you are not located in the united states\n",
            "you\n",
            "Input: the project gutenberg ebook of fausto primera parte\n",
            "    \n",
            "this ebook is for the use of anyone anywher\n",
            "Target: e\n",
            "Input shape: (100, 38)\n",
            "Target shape: (38,)\n",
            "================================================== \n",
            "\n",
            "Input: he project gutenberg ebook of fausto primera parte\n",
            "    \n",
            "this ebook is for the use of anyone anywhere\n",
            "Target:  \n",
            "Input shape: (100, 38)\n",
            "Target shape: (38,)\n",
            "================================================== \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m302,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m525,312\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)                  │           \u001b[38;5;34m9,766\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">302,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,766</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m837,158\u001b[0m (3.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">837,158</span> (3.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m837,158\u001b[0m (3.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">837,158</span> (3.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 33ms/step - accuracy: 0.3147 - loss: 2.2820\n",
            "Epoch 2/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.4541 - loss: 1.7455\n",
            "Epoch 3/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.5107 - loss: 1.5619\n",
            "Epoch 4/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.5507 - loss: 1.4332\n",
            "Epoch 5/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.5811 - loss: 1.3291\n",
            "Epoch 6/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.6068 - loss: 1.2377\n",
            "Epoch 7/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.6300 - loss: 1.1612\n",
            "Epoch 8/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.6514 - loss: 1.0901\n",
            "Epoch 9/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.6680 - loss: 1.0321\n",
            "Epoch 10/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.6836 - loss: 0.9813\n",
            "Epoch 11/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.6971 - loss: 0.9356\n",
            "Epoch 12/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.7077 - loss: 0.8981\n",
            "Epoch 13/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 34ms/step - accuracy: 0.7178 - loss: 0.8623\n",
            "Epoch 14/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.7294 - loss: 0.8326\n",
            "Epoch 15/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.7330 - loss: 0.8142\n",
            "Epoch 16/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.7442 - loss: 0.7759\n",
            "Epoch 17/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.7518 - loss: 0.7554\n",
            "Epoch 18/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.7563 - loss: 0.7386\n",
            "Epoch 19/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.7634 - loss: 0.7170\n",
            "Epoch 20/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.7670 - loss: 0.7004\n",
            "Epoch 21/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.7732 - loss: 0.6824\n",
            "Epoch 22/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.7779 - loss: 0.6632\n",
            "Epoch 23/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.7821 - loss: 0.6527\n",
            "Epoch 24/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 34ms/step - accuracy: 0.7871 - loss: 0.6397\n",
            "Epoch 25/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.7923 - loss: 0.6232\n",
            "Epoch 26/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.7945 - loss: 0.6138\n",
            "Epoch 27/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.7985 - loss: 0.6025\n",
            "Epoch 28/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.8034 - loss: 0.5861\n",
            "Epoch 29/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.8052 - loss: 0.5777\n",
            "Epoch 30/30\n",
            "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step - accuracy: 0.8098 - loss: 0.5665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from string import punctuation\n",
        "\n",
        "contenido = requests.get('https://www.gutenberg.org/cache/epub/68566/pg68566.txt').text\n",
        "open('data/fausto.txt', 'w', encoding=\"utf-8\").write(contenido)\n",
        "\n",
        "sequence_length = 100\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 30\n",
        "\n",
        "# Tenemos el path del archivo de datos\n",
        "\n",
        "FILE_PATH = \"data/fausto.txt\"\n",
        "BASENAME = os.path.basename(FILE_PATH)\n",
        "\n",
        "# Leemos la data\n",
        "text = open(FILE_PATH, encoding='utf-8').read()\n",
        "# removemos las letras mayusculas para un estilo mas uniforme\n",
        "text = text.lower()\n",
        "# removemos puntuacion\n",
        "text = text.translate(str.maketrans('', '', punctuation))\n",
        "text = text.translate(str.maketrans('', '', '¡ª«·»¿ßáäæèéëíñóöúü—‘’“”•™﻿'))\n",
        "\n",
        "\n",
        "# Esto nos ayuda basicamente a hacer que el entrenamiento sea mas rapido al reducir el vocabulario y hacer mas facil de digerir el texto\n",
        "\n",
        "# Veamos algunos stats del dataset\n",
        "\n",
        "n_chars = len(text)\n",
        "vocab = ''.join(sorted(set(text)))\n",
        "print('unique_chars:', vocab)\n",
        "n_unique_chars = len(vocab)\n",
        "print('Numero de caracteres:', n_chars)\n",
        "print('Numero de caracteres unicos:', n_unique_chars)\n",
        "\n",
        "# Hacemos dos diccionarios, dado que tenemos un string con todos los caracteres unicos de nuestro dataset, podemos hacer un diccionario que mapee a cada caracter un numero y viceversa\n",
        "\n",
        "char2int = {c: i for i, c in enumerate(vocab)}\n",
        "int2char = {i: c for i, c in enumerate(vocab)}\n",
        "\n",
        "# Los guardamos en un archivo\n",
        "pickle.dump(char2int, open(f'{BASENAME}-char2int.pickle', 'wb'))\n",
        "pickle.dump(int2char, open(f'{BASENAME}-int2char.pickle', 'wb'))\n",
        "\n",
        "# Ahora vamos a codificar nuestro dataset, o sea, convertir cada caracter en su entero correspondiente\n",
        "\n",
        "encoded_text = np.array([char2int[c] for c in text])\n",
        "\n",
        "# Construimos un tf.data.Dataset para nuestro encoded_text, si necesitamos escalar nuestro codigo a datasets mas grandes\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "\n",
        "for char in char_dataset.take(8):\n",
        "  print(char.numpy(), int2char[char.numpy()])\n",
        "\n",
        "# Ahora construimos nuestras oraciones, queremos que cada muestra de entrada sea la secuencia de caracteres de longitud sequence_length, y para eso usamos el metodo batch de tf.data.Dataset\n",
        "\n",
        "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
        "\n",
        "# Ahora lo mostramos\n",
        "\n",
        "for sequence in sequences.take(2):\n",
        "  print(''.join([int2char[i] for i in sequence.numpy()]))\n",
        "\n",
        "# Preparamos nuestros inputs y targets, necesitamos un modo de convertir una muestra (secuencia de caracteres) en multiples muestras. Y para eso podemos utilizar el metodo flat_map()\n",
        "\n",
        "def split_sample(sample):\n",
        "  ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
        "  for i in range(1, (len(sample)-1) // 2):\n",
        "    input_ = sample[i: i+sequence_length]\n",
        "    target = sample[i+sequence_length]\n",
        "    #Extendemos el datasete con concatenación\n",
        "    other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
        "    ds = ds.concatenate(other_ds)\n",
        "  return ds\n",
        "\n",
        "#Y ahora preparamos inputs y targets\n",
        "\n",
        "dataset = sequences.flat_map(split_sample)\n",
        "\n",
        "#Esto basicamente nos entrega una tupla de inputs y targets, donde conseguimos una gran cantidad de muestras de entrenamiento, y concatenamos para añadirlas juntas\n",
        "\n",
        "# Hagamos entonces one-hot code de los inputs y las labels (targets)\n",
        "\n",
        "def one_hot_samples(input_, target):\n",
        "  # Por ejemplo, de tener el caracter d (que se encuentra codificado como 3, con 5 caracteres unicos)\n",
        "  # Eso nos retorna el vector: [0, 0, 0, 1, 0], dado que 'd' es el 4to caracter\n",
        "  return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
        "\n",
        "dataset = dataset.map(one_hot_samples)\n",
        "\n",
        "# ahora hemos usado el conveniente metodo \"map()\", para hacer one-hot encode en cada muestra de nuestro dataset.\n",
        "\n",
        "# print las primeras 2 muestras\n",
        "for element in dataset.take(2):\n",
        "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
        "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
        "    print(\"Input shape:\", element[0].shape)\n",
        "    print(\"Target shape:\", element[1].shape)\n",
        "    print(\"=\"*50, \"\\n\")\n",
        "\n",
        "# repetimos, cambiamos y juntamos el dataset\n",
        "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True) # Con drop_remainder = True para eliminar las muestras con menor tamaño que el batch size\n",
        "\n",
        "# Armamos el modelo, el cual basicamente tiene dos capas LSTM con un numero de 128 unidades de LSTM arbitrario.\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(n_unique_chars, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "# Definimos el path del modelo\n",
        "\n",
        "model_weights_path = f'results/{BASENAME}-{sequence_length}.h5'\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Entrenamos al modelo\n",
        "\n",
        "#Hacemos la carpeta results si todavia no existe\n",
        "\n",
        "if not os.path.isdir(\"results\"):\n",
        "  os.mkdir(\"results\")\n",
        "\n",
        "#Entrenamos al modelo\n",
        "\n",
        "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)\n",
        "\n",
        "# Guardamos el modelo\n",
        "\n",
        "model.save(model_weights_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Activation\n",
        "import os\n",
        "\n",
        "sequence_length = 100\n",
        "# dataset file path\n",
        "FILE_PATH = \"data/fausto.txt\"\n",
        "# FILE_PATH = \"data/python_code.py\"\n",
        "BASENAME = os.path.basename(FILE_PATH)\n",
        "# Ahora probemos a generar nuevo texto\n",
        "\n",
        "# Como necesitamos una muestra, tomemos una semilla o alguna sentencia de la data de entrenamiento.\n",
        "\n",
        "seed = \"hermosas flores\"\n",
        "\n",
        "char2int = pickle.load(open(f'{BASENAME}-char2int.pickle', 'rb'))\n",
        "int2char = pickle.load(open(f'{BASENAME}-int2char.pickle', 'rb'))\n",
        "vocab_size = len(char2int)\n",
        "\n",
        "# Construimos el modelo nuevamente\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, vocab_size), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "#Y cargamos el set optimo de pesos del modelo.\n",
        "\n",
        "model.load_weights(f'results/{BASENAME}-{sequence_length}.h5')\n",
        "\n",
        "#y generamos\n",
        "\n",
        "s = seed\n",
        "n_chars = 400\n",
        "\n",
        "#Generamos 400 caracteres\n",
        "\n",
        "generated = ''\n",
        "\n",
        "for i in tqdm.tqdm(range(n_chars), 'Generando texto'):\n",
        "  #Hagamos la input de entrada\n",
        "  X = np.zeros((1, sequence_length, vocab_size))\n",
        "  for t, char in enumerate(seed):\n",
        "    X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1\n",
        "#Predecimos el siguiente caracter\n",
        "  predicted = model.predict(X, verbose=0)[0]\n",
        "  #Convertimos el vector a un entero\n",
        "  next_index = np.argmax(predicted)\n",
        "  #convertimos el entero a un caracter\n",
        "  next_char = int2char[next_index]\n",
        "  #añadimos el caracter a los resultados\n",
        "  generated += next_char\n",
        "  #Cambiamos la seed y el caracter predicho\n",
        "\n",
        "  seed = seed[1:] + next_char\n",
        "\n",
        "print('Seed:', s)\n",
        "print('Texto generado:')\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTfiLdrZKQMV",
        "outputId": "5227d3f9-5642-4186-b7b6-4dc20058025c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generando texto: 100%|██████████| 400/400 [00:25<00:00, 15.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: hermosas flores\n",
            "Texto generado:\n",
            " espritus el autor traducio fausto su alma que se aventura de fausto que no de toda la esperanza al tierno de aquello que han sido literarla\n",
            "  popular licortidad para que da apariencia del diablo sus pocos de la ciente con marta el tipo perdonad su autor se debin si li dictor si algn de todo el corazn humano la segunda parte de la tragedia la estengo clsica y asunto conceba en su seguid pues qued \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}